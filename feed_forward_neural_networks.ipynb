{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36be7176",
   "metadata": {},
   "source": [
    "This notebook references Lecture 7 from the Deep Learning course by Proffessor Bryce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e2196",
   "metadata": {},
   "source": [
    "This notebook implements a feed forward neural network with the following activation functions:\n",
    "1. Linear \n",
    "2. Logistic\n",
    "3. Tanh\n",
    "4. ReLu\n",
    "\n",
    "Then, Gradient Descent Technique is used to train the model.\n",
    "\n",
    "Regression Dataset -> https://archive.ics.uci.edu/dataset/360/air+quality \\\n",
    "Classification Dataset -> https://archive.ics.uci.edu/dataset/728/toxicity-2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938fe8f6",
   "metadata": {},
   "source": [
    "A single neuron is divided as the following in the larger picture.\n",
    "\n",
    "neuron = weighted average + activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df9f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da3904",
   "metadata": {},
   "source": [
    "### Sample Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d74e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data for regression and classification\n",
    "\n",
    "def synthetic_regression_data(N, # number of datapoints to generate\n",
    "                              n_input, # number of input variables \n",
    "                              ): \n",
    "    \n",
    "    # 1. determine weights and bias randomly for the data\n",
    "    W = np.random.randn(n_input)\n",
    "    b = np.random.randn()\n",
    "    \n",
    "    # 2. Generate data\n",
    "    X = np.random.randint(10, size=(N, n_input))\n",
    "    y = X.dot(W) + b + np.random.rand(N)\n",
    "    \n",
    "    \n",
    "    # 4. return the W, b and data\n",
    "    return X, y\n",
    "    \n",
    "    \n",
    "def synthetic_classification_data(N, n_input):\n",
    "    \n",
    "    # 1. determine weights and bias randomly for the data\n",
    "    W = np.random.randn(n_input)\n",
    "    b = np.random.randn()\n",
    "    \n",
    "    # 2. Generate data\n",
    "    X = np.random.randint(10, size=(N, n_input))\n",
    "    y_ = X.dot(W) + b + np.random.rand()\n",
    "    threshold = np.random.random()\n",
    "    y = (y_ >= threshold).astype(int)\n",
    "    \n",
    "    \n",
    "    # 4. return the W, b and data\n",
    "    return threshold, X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52658e",
   "metadata": {},
   "source": [
    "# Skeleton of Feed Forward Neural Network\n",
    "\n",
    "1. Input Layer \\\n",
    " => (N, M) where N is the number of data points & M is the number of input neurons \\\n",
    " => attribute = \"input\" (1-to-1 input mapping) \\\n",
    " => activation = None \n",
    "\n",
    " 2. Hidden Layers \\\n",
    " => (N, Hi) where Hi is the number of neurons in the ith layer \\\n",
    " => attribute = \"hidden\" \\\n",
    " => activation = linear/tanh/sigmoid/ReLu \n",
    "\n",
    " 3. Output Layer \\\n",
    " => (N, O) where O is the number of output neurons \\\n",
    " => attribute = \"output\" \\\n",
    " => activation = linear/sigmoid depending on the task type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9af0ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron():\n",
    "    \n",
    "    def __init__(self, n_features, activation = None):\n",
    "        self.activation = activation\n",
    "        self.W = np.random.randn(n_features)\n",
    "        self.b = np.random.randn()\n",
    "            \n",
    "    \n",
    "    def predict(self, X, train=None):\n",
    "        \n",
    "        y = self.linear_(X, self.W, self.b)\n",
    "        \n",
    "        if self.activation == \"linear\":\n",
    "            return y\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return self.sigmoid_(y)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return self.tanh_(y)\n",
    "        elif self.activation == \"ReLu\":\n",
    "            return self.relu_(y)\n",
    "        \n",
    "    \n",
    "    def sigmoid_(self, X):\n",
    "        return 1/ (1 + np.exp(-X))\n",
    "    \n",
    "    def tanh_(self, X):\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def relu_(self, X):\n",
    "        return np.maximum(0, X, axis=1)\n",
    "\n",
    "    def linear_(self, X, W, b):\n",
    "        return X.dot(W) + b\n",
    "    \n",
    "    def update_(self, W_grad, b_grad,alpha):\n",
    "        self.W -= (alpha * W_grad)\n",
    "        self.b -= (alpha * b_grad)\n",
    "    \n",
    "        \n",
    "class Layer():\n",
    "    \n",
    "    def __init__(self, size = (None, None, None), activation = None):\n",
    "        N, n_features, n_neurons = size\n",
    "        \n",
    "        self.size = size\n",
    "        self.activation = activation\n",
    "        self.neurons = {}\n",
    "        \n",
    "        for i in range(n_neurons):\n",
    "            self.neurons[f'neuron_{i+1}'] = Neuron(n_features, self.activation)\n",
    "                \n",
    "    def predict(self, X, train = None):\n",
    "        outputs = {}\n",
    "        i=0\n",
    "        for neuron_name, neuron in self.neurons.items():\n",
    "            \n",
    "            outputs[neuron_name] = np.array(neuron.predict(X, train)).reshape(-1, 1)\n",
    "            #print(outputs[neuron_name].shape)\n",
    "            \n",
    "        return np.hstack([*outputs.values()])\n",
    "    \n",
    "    def update(self, W_grad, b_grad, alpha):\n",
    "        for _, neuron in self.neurons.items():\n",
    "            neuron.update_(W_grad, b_grad, alpha)\n",
    "            \n",
    "            \n",
    "class RegressionModel:\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        \n",
    "        N, _ = input_size\n",
    "        \n",
    "        self.layer1 = Layer((*input_size, 3), activation=\"tanh\")\n",
    "        self.layer2 = Layer((N, 3, 2), activation=\"tanh\")\n",
    "        self.output_layer = Layer((N, 2, 1), activation=\"linear\")\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y = self.layer1.predict(X)\n",
    "        #print(y.shape)\n",
    "        y = self.layer2.predict(y)\n",
    "        #print(y.shape)\n",
    "        yo = self.output_layer.predict(y)\n",
    "        #print(yo.shape)\n",
    "        \n",
    "        return y, yo\n",
    "    \n",
    "    def update(self, W_grad, b_grad, alpha):\n",
    "        self.output_layer.update(W_grad, b_grad, alpha)\n",
    "        \n",
    "    def fit(self, X, y, alpha = 0.01, epochs = 100):\n",
    "        \n",
    "        m, n = X.shape\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 1. Compute Weighted Sum\n",
    "            yo, z = self.predict(X)\n",
    "            \n",
    "            error = y - z.reshape(-1, )\n",
    "\n",
    "            W_grad = (-2/m) * yo.T.dot(error)\n",
    "            b_grad = (-2/m) * np.sum(error)\n",
    "            \n",
    "            self.update(W_grad, b_grad, alpha)\n",
    "\n",
    "            #print(f\"Epoch {epoch+1} => \", np.mean(error ** 2))\n",
    "            \n",
    "            \n",
    "class ClassificationModel:\n",
    "    \n",
    "    def __init__(self, input_size, threshold):\n",
    "        \n",
    "        N, _ = input_size\n",
    "        self.threshold = threshold\n",
    "        self.layer1 = Layer((*input_size, 3), activation=\"tanh\")\n",
    "        self.layer2 = Layer((N, 3, 2), activation=\"tanh\")\n",
    "        self.output_layer = Layer((N, 2, 1), activation=\"sigmoid\")\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y = self.layer1.predict(X)\n",
    "        #print(y.shape)\n",
    "        y = self.layer2.predict(y)\n",
    "        #print(y.shape)\n",
    "        yo = self.output_layer.predict(y)\n",
    "        #print(yo.shape)\n",
    "        \n",
    "        return y, yo \n",
    "\n",
    "    def predict_(self, X):\n",
    "        y = self.layer1.predict(X)\n",
    "        print(y.shape)\n",
    "        y = self.layer2.predict(y)\n",
    "        print(y.shape)\n",
    "        y = self.output_layer.predict(y)\n",
    "        print(y.shape)\n",
    "        \n",
    "        return (y >= self.threshold).astype(int)\n",
    "    \n",
    "    def update(self, W_grad, b_grad, alpha):\n",
    "        self.output_layer.update(W_grad, b_grad, alpha)\n",
    "        \n",
    "    def fit(self, X, y, alpha = 0.01, epochs = 100):\n",
    "        \n",
    "        m, n = X.shape\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 1. Compute Weighted Sum\n",
    "            yo, z = self.predict(X)\n",
    "            \n",
    "            error = y - z.reshape(-1, )\n",
    "\n",
    "            W_grad = (-2/m) * yo.T.dot(error)\n",
    "            b_grad = (-2/m) * np.sum(error)\n",
    "            \n",
    "            self.update(W_grad, b_grad, alpha)\n",
    "\n",
    "            #print(f\"Epoch {epoch+1} => \", np.mean(error ** 2))\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f26ba5",
   "metadata": {},
   "source": [
    "Testing on synthetic regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f2a4627e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit complete\n"
     ]
    }
   ],
   "source": [
    "X_reg, y_reg = synthetic_regression_data(100, 5)\n",
    "\n",
    "reg_model = RegressionModel((100, 5))\n",
    "\n",
    "reg_model.fit(X_reg, y_reg)\n",
    "print(\"Fit complete\")\n",
    "\n",
    "_, y_pred = reg_model.predict(X_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df371c08",
   "metadata": {},
   "source": [
    "Testing on synthetic classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ec38eaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit complete\n",
      "(100, 3)\n",
      "(100, 2)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "cls_threshold, X_cls, y_cls = synthetic_classification_data(100, 5)\n",
    "\n",
    "cls_model = ClassificationModel((100, 5), 0.5)\n",
    "\n",
    "cls_model.fit(X_cls, y_cls)\n",
    "print(\"Fit complete\")\n",
    "\n",
    "y_pred = cls_model.predict_(X_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d1fccf",
   "metadata": {},
   "source": [
    "Testing on real datasets mentioned earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6bf9e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetching classification dataset \n",
    "toxicity = fetch_ucirepo(id=728) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_toxic = toxicity.data.features \n",
    "y_toxic = toxicity.data.targets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4743d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity sake we are skipping all the categorical features and considering only around 10 random features\n",
    "\n",
    "toxicity_features = ['MATS3v', 'MATS3s', 'MATS3p', 'nHBDon_Lipinski', 'minHBint8', 'MATS3e', 'MATS3c', 'MATS3m'] \n",
    "X_toxic = X_toxic[toxicity_features]\n",
    "\n",
    "y_toxic = (np.array(y_toxic) != \"NonToxic\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89f3ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as accsc, mean_squared_error as mse, r2_score as r2, precision_score as presc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e53a91be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 3)\n",
      "(171, 2)\n",
      "(171, 1)\n",
      "Accuracy score :  0.5087719298245614\n",
      "Precision score :  0.35714285714285715\n"
     ]
    }
   ],
   "source": [
    "cls_model_real = ClassificationModel(X_toxic.shape, 0.6)\n",
    "cls_model_real.fit(X_toxic, np.asarray(y_toxic).reshape(-1, ))\n",
    "y_cls_model_real = cls_model_real.predict_(X_toxic)\n",
    "\n",
    "print(\"Accuracy score : \", accsc(y_toxic, y_cls_model_real))\n",
    "print(\"Precision score : \", presc(y_toxic, y_cls_model_real))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "93ae424c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45730 entries, 0 to 45729\n",
      "Data columns (total 10 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   RMSD    45730 non-null  float64\n",
      " 1   F1      45730 non-null  float64\n",
      " 2   F2      45730 non-null  float64\n",
      " 3   F3      45730 non-null  float64\n",
      " 4   F4      45730 non-null  float64\n",
      " 5   F5      45730 non-null  float64\n",
      " 6   F6      45730 non-null  float64\n",
      " 7   F7      45730 non-null  float64\n",
      " 8   F8      45730 non-null  int64  \n",
      " 9   F9      45730 non-null  float64\n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 3.5 MB\n"
     ]
    }
   ],
   "source": [
    "# fetching classification dataset https://archive.ics.uci.edu/dataset/265/physicochemical+properties+of+protein+tertiary+structure\n",
    "\n",
    "reg_data = pd.read_csv('./datasets/CASP.csv')\n",
    "reg_data.info()\n",
    "\n",
    "X_casp, y_casp = reg_data.drop(['RMSD'], axis=1), reg_data['RMSD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "87403b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score :  -2.5108894369907375e-05\n",
      "MSE :  37.43385907519307\n"
     ]
    }
   ],
   "source": [
    "reg_model_real = RegressionModel(X_casp.shape)\n",
    "reg_model_real.fit(X_casp, np.asarray(y_casp), epochs=100)\n",
    "_, y_reg_model_real = reg_model_real.predict(X_casp)\n",
    "\n",
    "print(\"R2 score : \", r2(y_casp, y_reg_model_real))\n",
    "print(\"MSE : \", mse(y_casp, y_reg_model_real))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11647f9d",
   "metadata": {},
   "source": [
    "Important learning here:\n",
    "\n",
    "\n",
    "the version implemented here uses exponential to compute tanh.\n",
    "But for larger values, tanh tends to infinty resulting Nan values and its propagation.\n",
    "\n",
    "Thus it is important to keep this in mind while building this activation function.\n",
    "\n",
    "The manual implementatio is replaced with the numpy version for stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
